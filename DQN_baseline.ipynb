{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    #define hyper parameter\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.gamma = 0.95 #reward discount rate\n",
    "        self.epsilon = 1.0 #exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    #build Deep Q learning model\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim = self.state_size))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(self.action_size, activation = 'linear'))\n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    #to explore new action, or exploit the action that lead to maxmize \n",
    "    def act(self, state):\n",
    "        # explore\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        #return action that lead to biggest predicted reward\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0]) \n",
    "        \n",
    "        \n",
    "        \n",
    "    #create replay memory\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "   \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            #calculate the target Q values\n",
    "            if done == True:\n",
    "                target_q = reward \n",
    "            else:\n",
    "                target_q = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "        \n",
    "            \n",
    "            #calculate the output Q values\n",
    "            output_q = self.model.predict(state)\n",
    "            #replace with target action_state pair\n",
    "            output_q[0][action] = target_q\n",
    "    \n",
    "            #fit the weights to the data\n",
    "            self.model.fit(state, output_q, epochs = 1, verbose = 0)\n",
    "        \n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-12 14:53:41,206] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5000, score: 17\n",
      "episode: 1/5000, score: 14\n",
      "episode: 2/5000, score: 11\n",
      "episode: 3/5000, score: 9\n",
      "episode: 4/5000, score: 9\n",
      "episode: 5/5000, score: 9\n",
      "episode: 6/5000, score: 8\n",
      "episode: 7/5000, score: 9\n",
      "episode: 8/5000, score: 8\n",
      "episode: 9/5000, score: 8\n",
      "episode: 10/5000, score: 8\n",
      "episode: 11/5000, score: 10\n",
      "episode: 12/5000, score: 9\n",
      "episode: 13/5000, score: 7\n",
      "episode: 14/5000, score: 9\n",
      "episode: 15/5000, score: 8\n",
      "episode: 16/5000, score: 8\n",
      "episode: 17/5000, score: 8\n",
      "episode: 18/5000, score: 8\n",
      "episode: 19/5000, score: 60\n",
      "episode: 20/5000, score: 62\n",
      "episode: 21/5000, score: 85\n",
      "episode: 22/5000, score: 139\n",
      "episode: 23/5000, score: 47\n",
      "episode: 24/5000, score: 53\n",
      "episode: 25/5000, score: 30\n",
      "episode: 26/5000, score: 53\n",
      "episode: 27/5000, score: 36\n",
      "episode: 28/5000, score: 39\n",
      "episode: 29/5000, score: 88\n",
      "episode: 30/5000, score: 41\n",
      "episode: 31/5000, score: 25\n",
      "episode: 32/5000, score: 36\n",
      "episode: 33/5000, score: 95\n",
      "episode: 34/5000, score: 28\n",
      "episode: 35/5000, score: 44\n",
      "episode: 36/5000, score: 25\n",
      "episode: 37/5000, score: 43\n",
      "episode: 38/5000, score: 199\n",
      "episode: 39/5000, score: 199\n",
      "episode: 40/5000, score: 199\n",
      "episode: 41/5000, score: 199\n",
      "episode: 42/5000, score: 199\n",
      "episode: 43/5000, score: 199\n",
      "episode: 44/5000, score: 199\n",
      "episode: 45/5000, score: 199\n",
      "episode: 46/5000, score: 197\n",
      "episode: 47/5000, score: 121\n",
      "episode: 48/5000, score: 54\n",
      "episode: 49/5000, score: 172\n",
      "episode: 50/5000, score: 199\n",
      "episode: 51/5000, score: 136\n",
      "episode: 52/5000, score: 13\n",
      "episode: 53/5000, score: 199\n",
      "episode: 54/5000, score: 199\n",
      "episode: 55/5000, score: 199\n",
      "episode: 56/5000, score: 199\n",
      "episode: 57/5000, score: 185\n",
      "episode: 58/5000, score: 199\n",
      "episode: 59/5000, score: 199\n",
      "episode: 60/5000, score: 199\n",
      "episode: 61/5000, score: 195\n",
      "episode: 62/5000, score: 199\n",
      "episode: 63/5000, score: 199\n",
      "episode: 64/5000, score: 74\n",
      "episode: 65/5000, score: 150\n",
      "episode: 66/5000, score: 170\n",
      "episode: 67/5000, score: 67\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #initialize environment and the agent\n",
    "    env = gym.make('CartPole-v0')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    \n",
    "    \n",
    "    #iterate the game\n",
    "    for e in range(5000):\n",
    "        #reset state \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state,[1,4])\n",
    "    \n",
    "        #iterate the time step\n",
    "        for time_step in range(500):\n",
    "            #env.render()\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1,4])\n",
    "            \n",
    "            #put experience to the replay memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            #make next state the current state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, 5000, time_step))\n",
    "                break\n",
    "            \n",
    "            if len(agent.memory) > batch_size: \n",
    "                agent.replay(batch_size)\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
