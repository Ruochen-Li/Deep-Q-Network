{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    #define hyper parameter\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen = 2000)\n",
    "        self.gamma = 0.95 #reward discount rate\n",
    "        self.epsilon = 1.0 #exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    #build Deep Q learning model\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim = self.state_size))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(self.action_size, activation = 'linear'))\n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    #to explore new action, or exploit the action that lead to maxmize \n",
    "    def act(self, state):\n",
    "        # explore\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        #return action that lead to biggest predicted reward\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0]) \n",
    "        \n",
    "        \n",
    "        \n",
    "    #create replay memory\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "   \n",
    "    def replay(self, batch_size):\n",
    "        #calculate time-difference error\n",
    "        #random sample is replaced by rank or propotionally sampling method\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            #calculate the target Q values\n",
    "            if done == True:\n",
    "                target_q = reward \n",
    "            else:\n",
    "                target_q = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "        \n",
    "            \n",
    "            #calculate the output Q values\n",
    "            output_q = self.model.predict(state)\n",
    "            #replace with target action_state pair\n",
    "            output_q[0][action] = target_q\n",
    "    \n",
    "            #fit the weights to the data, minimize the difference between target-q and output_q\n",
    "            self.model.fit(state, output_q, epochs = 1, verbose = 0)\n",
    "        \n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-19 10:21:08,502] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/5000, score: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-06-19 10:21:09,152] From C:\\Users\\asus\\Anaconda3\\envs\\PythonCPU\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/5000, score: 32\n",
      "episode: 2/5000, score: 55\n",
      "episode: 3/5000, score: 37\n",
      "episode: 4/5000, score: 46\n",
      "episode: 5/5000, score: 40\n",
      "episode: 6/5000, score: 38\n",
      "episode: 7/5000, score: 59\n",
      "episode: 8/5000, score: 47\n",
      "episode: 9/5000, score: 163\n",
      "episode: 10/5000, score: 67\n",
      "episode: 11/5000, score: 81\n",
      "episode: 12/5000, score: 81\n",
      "episode: 13/5000, score: 96\n",
      "episode: 14/5000, score: 79\n",
      "episode: 15/5000, score: 106\n",
      "episode: 16/5000, score: 190\n",
      "episode: 17/5000, score: 199\n",
      "episode: 18/5000, score: 199\n",
      "episode: 19/5000, score: 199\n",
      "episode: 20/5000, score: 199\n",
      "episode: 21/5000, score: 199\n",
      "episode: 22/5000, score: 128\n",
      "episode: 23/5000, score: 108\n",
      "episode: 24/5000, score: 86\n",
      "episode: 25/5000, score: 95\n",
      "episode: 26/5000, score: 21\n",
      "episode: 27/5000, score: 17\n",
      "episode: 28/5000, score: 167\n",
      "episode: 29/5000, score: 112\n",
      "episode: 30/5000, score: 110\n",
      "episode: 31/5000, score: 139\n",
      "episode: 32/5000, score: 118\n",
      "episode: 33/5000, score: 151\n",
      "episode: 34/5000, score: 59\n",
      "episode: 35/5000, score: 145\n",
      "episode: 36/5000, score: 146\n",
      "episode: 37/5000, score: 179\n",
      "episode: 38/5000, score: 112\n",
      "episode: 39/5000, score: 41\n",
      "episode: 40/5000, score: 199\n",
      "episode: 41/5000, score: 199\n",
      "episode: 42/5000, score: 199\n",
      "episode: 43/5000, score: 199\n",
      "episode: 44/5000, score: 199\n",
      "episode: 45/5000, score: 199\n",
      "episode: 46/5000, score: 199\n",
      "episode: 47/5000, score: 199\n",
      "episode: 48/5000, score: 199\n",
      "episode: 49/5000, score: 199\n",
      "episode: 50/5000, score: 189\n",
      "episode: 51/5000, score: 194\n",
      "episode: 52/5000, score: 56\n",
      "episode: 53/5000, score: 199\n",
      "episode: 54/5000, score: 168\n",
      "episode: 55/5000, score: 41\n",
      "episode: 56/5000, score: 25\n",
      "episode: 57/5000, score: 55\n",
      "episode: 58/5000, score: 84\n",
      "episode: 59/5000, score: 8\n",
      "episode: 60/5000, score: 114\n",
      "episode: 61/5000, score: 199\n",
      "episode: 62/5000, score: 199\n",
      "episode: 63/5000, score: 158\n",
      "episode: 64/5000, score: 199\n",
      "episode: 65/5000, score: 199\n",
      "episode: 66/5000, score: 199\n",
      "episode: 67/5000, score: 199\n",
      "episode: 68/5000, score: 199\n",
      "episode: 69/5000, score: 128\n",
      "episode: 70/5000, score: 176\n",
      "episode: 71/5000, score: 199\n",
      "episode: 72/5000, score: 199\n",
      "episode: 73/5000, score: 165\n",
      "episode: 74/5000, score: 149\n",
      "episode: 75/5000, score: 199\n",
      "episode: 76/5000, score: 199\n",
      "episode: 77/5000, score: 134\n",
      "episode: 78/5000, score: 199\n",
      "episode: 79/5000, score: 199\n",
      "episode: 80/5000, score: 37\n",
      "episode: 81/5000, score: 199\n",
      "episode: 82/5000, score: 199\n",
      "episode: 83/5000, score: 199\n",
      "episode: 84/5000, score: 119\n",
      "episode: 85/5000, score: 179\n",
      "episode: 86/5000, score: 178\n",
      "episode: 87/5000, score: 199\n",
      "episode: 88/5000, score: 187\n",
      "episode: 89/5000, score: 199\n",
      "episode: 90/5000, score: 199\n",
      "episode: 91/5000, score: 66\n",
      "episode: 92/5000, score: 80\n",
      "episode: 93/5000, score: 57\n",
      "episode: 94/5000, score: 199\n",
      "episode: 95/5000, score: 186\n",
      "episode: 96/5000, score: 9\n",
      "episode: 97/5000, score: 199\n",
      "episode: 98/5000, score: 15\n",
      "episode: 99/5000, score: 73\n",
      "episode: 100/5000, score: 199\n",
      "episode: 101/5000, score: 144\n",
      "episode: 102/5000, score: 199\n",
      "episode: 103/5000, score: 135\n",
      "episode: 104/5000, score: 199\n",
      "episode: 105/5000, score: 199\n",
      "episode: 106/5000, score: 199\n",
      "episode: 107/5000, score: 160\n",
      "episode: 108/5000, score: 154\n",
      "episode: 109/5000, score: 198\n",
      "episode: 110/5000, score: 123\n",
      "episode: 111/5000, score: 95\n",
      "episode: 112/5000, score: 12\n",
      "episode: 113/5000, score: 12\n",
      "episode: 114/5000, score: 109\n",
      "episode: 115/5000, score: 170\n",
      "episode: 116/5000, score: 114\n",
      "episode: 117/5000, score: 182\n",
      "episode: 118/5000, score: 82\n",
      "episode: 119/5000, score: 9\n",
      "episode: 120/5000, score: 8\n",
      "episode: 121/5000, score: 9\n",
      "episode: 122/5000, score: 144\n",
      "episode: 123/5000, score: 175\n",
      "episode: 124/5000, score: 130\n",
      "episode: 125/5000, score: 150\n",
      "episode: 126/5000, score: 137\n",
      "episode: 127/5000, score: 124\n",
      "episode: 128/5000, score: 99\n",
      "episode: 129/5000, score: 199\n",
      "episode: 130/5000, score: 162\n",
      "episode: 131/5000, score: 199\n",
      "episode: 132/5000, score: 199\n",
      "episode: 133/5000, score: 199\n",
      "episode: 134/5000, score: 199\n",
      "episode: 135/5000, score: 156\n",
      "episode: 136/5000, score: 23\n",
      "episode: 137/5000, score: 111\n",
      "episode: 138/5000, score: 199\n",
      "episode: 139/5000, score: 139\n",
      "episode: 140/5000, score: 8\n",
      "episode: 141/5000, score: 8\n",
      "episode: 142/5000, score: 8\n",
      "episode: 143/5000, score: 9\n",
      "episode: 144/5000, score: 7\n",
      "episode: 145/5000, score: 8\n",
      "episode: 146/5000, score: 8\n",
      "episode: 147/5000, score: 9\n",
      "episode: 148/5000, score: 9\n",
      "episode: 149/5000, score: 8\n",
      "episode: 150/5000, score: 8\n",
      "episode: 151/5000, score: 10\n",
      "episode: 152/5000, score: 8\n",
      "episode: 153/5000, score: 9\n",
      "episode: 154/5000, score: 8\n",
      "episode: 155/5000, score: 10\n",
      "episode: 156/5000, score: 24\n",
      "episode: 157/5000, score: 23\n",
      "episode: 158/5000, score: 20\n",
      "episode: 159/5000, score: 20\n",
      "episode: 160/5000, score: 19\n",
      "episode: 161/5000, score: 17\n",
      "episode: 162/5000, score: 13\n",
      "episode: 163/5000, score: 13\n",
      "episode: 164/5000, score: 11\n",
      "episode: 165/5000, score: 13\n",
      "episode: 166/5000, score: 16\n",
      "episode: 167/5000, score: 13\n",
      "episode: 168/5000, score: 10\n",
      "episode: 169/5000, score: 13\n",
      "episode: 170/5000, score: 20\n",
      "episode: 171/5000, score: 12\n",
      "episode: 172/5000, score: 14\n",
      "episode: 173/5000, score: 17\n",
      "episode: 174/5000, score: 16\n",
      "episode: 175/5000, score: 21\n",
      "episode: 176/5000, score: 23\n",
      "episode: 177/5000, score: 135\n",
      "episode: 178/5000, score: 149\n",
      "episode: 179/5000, score: 199\n",
      "episode: 180/5000, score: 199\n",
      "episode: 181/5000, score: 199\n",
      "episode: 182/5000, score: 199\n",
      "episode: 183/5000, score: 199\n",
      "episode: 184/5000, score: 199\n",
      "episode: 185/5000, score: 199\n",
      "episode: 186/5000, score: 199\n",
      "episode: 187/5000, score: 191\n",
      "episode: 188/5000, score: 92\n",
      "episode: 189/5000, score: 69\n",
      "episode: 190/5000, score: 12\n",
      "episode: 191/5000, score: 54\n",
      "episode: 192/5000, score: 104\n",
      "episode: 193/5000, score: 37\n",
      "episode: 194/5000, score: 129\n",
      "episode: 195/5000, score: 131\n",
      "episode: 196/5000, score: 30\n",
      "episode: 197/5000, score: 12\n",
      "episode: 198/5000, score: 175\n",
      "episode: 199/5000, score: 67\n",
      "episode: 200/5000, score: 115\n",
      "episode: 201/5000, score: 129\n",
      "episode: 202/5000, score: 104\n",
      "episode: 203/5000, score: 136\n",
      "episode: 204/5000, score: 121\n",
      "episode: 205/5000, score: 199\n",
      "episode: 206/5000, score: 166\n",
      "episode: 207/5000, score: 195\n",
      "episode: 208/5000, score: 194\n",
      "episode: 209/5000, score: 164\n",
      "episode: 210/5000, score: 199\n",
      "episode: 211/5000, score: 122\n",
      "episode: 212/5000, score: 32\n",
      "episode: 213/5000, score: 11\n",
      "episode: 214/5000, score: 199\n",
      "episode: 215/5000, score: 121\n",
      "episode: 216/5000, score: 125\n",
      "episode: 217/5000, score: 199\n",
      "episode: 218/5000, score: 199\n",
      "episode: 219/5000, score: 64\n",
      "episode: 220/5000, score: 94\n",
      "episode: 221/5000, score: 199\n",
      "episode: 222/5000, score: 199\n",
      "episode: 223/5000, score: 199\n",
      "episode: 224/5000, score: 199\n",
      "episode: 225/5000, score: 199\n",
      "episode: 226/5000, score: 175\n",
      "episode: 227/5000, score: 199\n",
      "episode: 228/5000, score: 199\n",
      "episode: 229/5000, score: 199\n",
      "episode: 230/5000, score: 199\n",
      "episode: 231/5000, score: 131\n",
      "episode: 232/5000, score: 199\n",
      "episode: 233/5000, score: 199\n",
      "episode: 234/5000, score: 199\n",
      "episode: 235/5000, score: 199\n",
      "episode: 236/5000, score: 199\n",
      "episode: 237/5000, score: 199\n",
      "episode: 238/5000, score: 40\n",
      "episode: 239/5000, score: 177\n",
      "episode: 240/5000, score: 137\n",
      "episode: 241/5000, score: 125\n",
      "episode: 242/5000, score: 170\n",
      "episode: 243/5000, score: 179\n",
      "episode: 244/5000, score: 105\n",
      "episode: 245/5000, score: 55\n",
      "episode: 246/5000, score: 139\n",
      "episode: 247/5000, score: 36\n",
      "episode: 248/5000, score: 44\n",
      "episode: 249/5000, score: 112\n",
      "episode: 250/5000, score: 199\n",
      "episode: 251/5000, score: 194\n",
      "episode: 252/5000, score: 199\n",
      "episode: 253/5000, score: 154\n",
      "episode: 254/5000, score: 199\n",
      "episode: 255/5000, score: 38\n",
      "episode: 256/5000, score: 59\n",
      "episode: 257/5000, score: 17\n",
      "episode: 258/5000, score: 62\n",
      "episode: 259/5000, score: 10\n",
      "episode: 260/5000, score: 23\n",
      "episode: 261/5000, score: 199\n",
      "episode: 262/5000, score: 199\n",
      "episode: 263/5000, score: 199\n",
      "episode: 264/5000, score: 199\n",
      "episode: 265/5000, score: 148\n",
      "episode: 266/5000, score: 114\n",
      "episode: 267/5000, score: 160\n",
      "episode: 268/5000, score: 127\n",
      "episode: 269/5000, score: 176\n",
      "episode: 270/5000, score: 78\n",
      "episode: 271/5000, score: 127\n",
      "episode: 272/5000, score: 199\n",
      "episode: 273/5000, score: 199\n",
      "episode: 274/5000, score: 144\n",
      "episode: 275/5000, score: 195\n",
      "episode: 276/5000, score: 190\n",
      "episode: 277/5000, score: 196\n",
      "episode: 278/5000, score: 35\n",
      "episode: 279/5000, score: 14\n",
      "episode: 280/5000, score: 99\n",
      "episode: 281/5000, score: 199\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #initialize environment and the agent\n",
    "    env = gym.make('CartPole-v0')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    \n",
    "    \n",
    "    #iterate the game\n",
    "    for e in range(5000):\n",
    "        #reset state \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state,[1,4])\n",
    "    \n",
    "        #iterate the time step\n",
    "        for time_step in range(500):\n",
    "            #env.render()\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1,4])\n",
    "            \n",
    "            #put experience to the replay memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            #make next state the current state\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                print(\"episode: {}/{}, score: {}\".format(e, 5000, time_step))\n",
    "                break\n",
    "            \n",
    "            #training with replay memory\n",
    "            if len(agent.memory) > batch_size: \n",
    "                agent.replay(batch_size)\n",
    "                \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
